{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/ubuntu/CUAI_2021/Advanced_Minkyu_Kim/Scalp_Health_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalp_Health_Dataset(Dataset) :\n",
    "    def __init__(self, image_path_list, label_list) : # 용량을 고려해 이미지는 경로만 받는걸로\n",
    "        self.image_path_list = image_path_list\n",
    "        self.label_list = label_list\n",
    "    def __len__(self) : \n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, index) : # 한 개의 데이터 가져오는 함수\n",
    "        \n",
    "        # 224 X 224로 전처리\n",
    "        img = (Image.open(self.image_path_list[index]).convert('RGB'))\n",
    "        img_transform = transforms.Compose([transforms.PILToTensor(), transforms.Resize((224, 224))]) # '텐서로 변경 후 이미지 리사이징'하는 연산을 생성\n",
    "        img = torch.divide(img_transform(img), 255) # 텐서로 변경 후 이미지 리사이징하고 각 채널을 0~1 사이의 값으로 만들어버림\n",
    "        \n",
    "        # 출력값은 각 항목(Value1~Value6)은 3으로 나눈 뒤 출력. (각 항목의 최대값이 3)\n",
    "        # 모델에서 출력값을 얻고 3씩 곱해주면 된다\n",
    "        \n",
    "        label = self.label_list[index]/3.0\n",
    "        \n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataset_path, category) : # root_path + '/Train'이나 root_path + '/Label'을 받음\n",
    "    \n",
    "    \n",
    "    image_group_folder_path = dataset_path + '/Image'\n",
    "    label_group_folder_path = dataset_path + '/Label'\n",
    "    \n",
    "    ori_label_folder_list = os.listdir(label_group_folder_path) # '[라벨]피지과다_3.중증' 등 폴더명 알기\n",
    "    \n",
    "    label_folder_list = []\n",
    "    \n",
    "    for i in range(len(ori_label_folder_list)) :\n",
    "        if ori_label_folder_list[i] != '.DS_Store' : # '.DS_Store'가 생성되었을 수 있으니 폴더 목록에서 제외\n",
    "            label_folder_list.append(ori_label_folder_list[i])\n",
    "    \n",
    "    image_path_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    desc_str = category + \"_make_dataset\"\n",
    "    \n",
    "    for i in tqdm(range(len(label_folder_list)), desc = desc_str) :\n",
    "                  \n",
    "        label_folder_path = label_group_folder_path + \"/\" + label_folder_list[i]\n",
    "        \n",
    "        # label_folder_list에서 '라벨'을 '원천'으로 만 바꿔도 image파일들이 들어있는 폴더명으로 만들 수 있다\n",
    "        image_folder_path = image_group_folder_path + \"/\" + label_folder_list[i].replace('라벨', '원천')\n",
    "        \n",
    "        json_list = os.listdir(label_folder_path) # json파일 목록 담기\n",
    "\n",
    "        for j in range(len(json_list)) : \n",
    "            json_file_path = label_folder_path + '/' + json_list[j]\n",
    "\n",
    "            with open(json_file_path, \"r\", encoding=\"utf8\") as f: \n",
    "                contents = f.read() # string 타입 \n",
    "                json_content = json.loads(contents) # 딕셔너리로 저장\n",
    "\n",
    "            image_file_name = json_content['image_file_name'] # 라벨 데이터에 이미지 파일의 이름이 들어있다\n",
    "            \n",
    "            image_file_path = image_folder_path + \"/\" + image_file_name\n",
    "\n",
    "            y_true = []\n",
    "            y_true.append(int(json_content['value_1']))\n",
    "            y_true.append(int(json_content['value_2']))\n",
    "            y_true.append(int(json_content['value_3']))\n",
    "            y_true.append(int(json_content['value_4']))\n",
    "            y_true.append(int(json_content['value_5']))\n",
    "            y_true.append(int(json_content['value_6']))\n",
    "\n",
    "            y_true = np.asarray(y_true)\n",
    "\n",
    "            image_path_list.append(image_file_path)\n",
    "            label_list.append(y_true)\n",
    "\n",
    "    return image_path_list, label_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(root_path) : \n",
    "    \n",
    "    Train_image_path_list, Train_label_list = make_dataset(root_path + '/Train', \"Train\")\n",
    "    Test_image_path_list, Test_label_list = make_dataset(root_path + '/Test', \"Test\")\n",
    "    \n",
    "    Train_Scalp_Health_Dataset = Scalp_Health_Dataset(Train_image_path_list, Train_label_list)\n",
    "    Test_Scalp_Health_Dataset = Scalp_Health_Dataset(Test_image_path_list, Test_label_list)\n",
    "    \n",
    "    return Train_Scalp_Health_Dataset, Test_Scalp_Health_Dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/CUAI_2021/Advanced_Minkyu_Kim/sampled_dataset/Train/Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3eff3ade9005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrain_Scalp_Health_Dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_Scalp_Health_Dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-493833188628>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(root_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mTrain_image_path_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mTest_image_path_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-382a44458ebe>\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(dataset_path, category)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabel_group_folder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Label'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mori_label_folder_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_group_folder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# '[라벨]피지과다_3.중증' 등 폴더명 알기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlabel_folder_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/CUAI_2021/Advanced_Minkyu_Kim/sampled_dataset/Train/Label'"
     ]
    }
   ],
   "source": [
    "Train_Scalp_Health_Dataset, Test_Scalp_Health_Dataset = get_dataset(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=Train_Scalp_Health_Dataset, # 사용할 데이터셋\n",
    "                                          batch_size=BATCH_SIZE, # 미니배치 크기\n",
    "                                          shuffle=True, # 에포크마다 데이터셋 셔플할건가? \n",
    "                                          drop_last=True) # 마지막 배치가 BATCH_SIZE보다 작을 수 있다. 나머지가 항상 0일 수는 없지 않는가. 이 때 마지막 배치는 사용하지 않으려면 drop_last = True를, 사용할거면 drop_last = False를 입력한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 생성 : EfficientNet_b4에 classifier만 수정해서 학습시킨다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/minkyukim/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/Users/minkyukim/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/Users/minkyukim/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b4', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모발 분류를 위한 linear model 생성 후 교체\n",
    "new_classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2, inplace=False),\n",
    "            nn.Linear(in_features=1280, out_features=512, bias=True),\n",
    "            nn.Linear(in_features=512, out_features=256, bias=True),\n",
    "            nn.Linear(in_features=256, out_features=6, bias=True)\n",
    "        )\n",
    "\n",
    "for m in new_classifier.modules():\n",
    "    if isinstance(m, nn.Linear) :\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        \n",
    "model.classifier = new_classifier # 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1154)\n"
     ]
    }
   ],
   "source": [
    "# l2_reg = 0\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         l2_reg += torch.norm(param.data)\n",
    "\n",
    "# l2_reg = 1e-5 * l2_reg\n",
    "# print(l2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.256, alpha=0.99, eps=1e-08, weight_decay=1e-5, momentum=0.9, centered=False)\n",
    "EPOCHS = 400\n",
    "loss = nn.CrossEntropyLoss() # lienar model만 학습시킬거라 CrossEntropyLoss만 사용. Conv들은 수정을 하지 않을 것\n",
    "\n",
    "# 디바이스 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, epochs, data_loader, device) :\n",
    "    # 학습 -> 성능 측정\n",
    "\n",
    "    model.eval()\n",
    "    model.classifier.train()\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"training\", mininterval=0.01)\n",
    "\n",
    "    for epoch in pbar:  # loop over the dataset multiple times\n",
    "        \n",
    "        for inputs, labels in data_loader:\n",
    "\n",
    "            # get the inputs\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            l2_reg = 0\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    l2_reg += torch.norm(param.data)\n",
    "\n",
    "            loss = torch.add(loss,  1e-5 * l2_reg)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            pbar_str = \"training, [loss = %.4f]\" % loss.item()\n",
    "            pbar.set_description(pbar_str)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, loss, optimizer, EPOCHS, data_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 (추후 구현 에정)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aac1938eb6913997aabd086dca6066ca74adbc6b4a9aafd1e6821241d3ac0498"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
